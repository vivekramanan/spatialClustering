{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0f45b2",
   "metadata": {},
   "source": [
    "# Geospatial Clustering \n",
    "\n",
    "Based on two clustering methods\n",
    "1) Local Morans\n",
    "2) Markov Clustering Linkage\n",
    "\n",
    "And runs feature importance for both methods. \n",
    "\n",
    "### Before running this notebook, make sure the following steps were done:\n",
    "1. myVenv environment was activated\n",
    "2. All required dependencies were installed with \"pip install -r requirements.txt\"\n",
    "3. jupyter was opened in the myVenv environment \n",
    "\n",
    "## To run every cell, press shift+enter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import rand_score\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pysal.lib import weights\n",
    "from pysal.lib import weights\n",
    "from splot.libpysal import plot_spatial_weights\n",
    "from esda.moran import Moran, Moran_Local\n",
    "from splot.esda import moran_scatterplot, plot_local_autocorrelation, lisa_cluster\n",
    "import folium\n",
    "import matplotlib.colors as colors\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "import markov_clustering as mc\n",
    "from scipy.sparse import csr_matrix\n",
    "import kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91249d42",
   "metadata": {},
   "source": [
    "If the previous cell gives a \"ModuleNotFoundError\", run the following command: \n",
    "\n",
    "pip install insert_module_name\n",
    "\n",
    "In the case of geopandas specifically, run the following command on command line: \n",
    "\n",
    "conda install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf746d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module Installation if needed:\n",
    "\n",
    "# pip install insert_module_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5532aaf",
   "metadata": {},
   "source": [
    "## Step 1: Input the state of choice below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2991edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateName = \"MA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68611043",
   "metadata": {},
   "source": [
    "## Step 2: Essential Functions \n",
    "\n",
    "When you run these, there is no output until you actually call the function later below. \n",
    "These functions merge together the EJI and zipcode mapping functions for the specific state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb86bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeDFs(eji, zipT): \n",
    "    \"\"\"\n",
    "    Merged two dataframes of EJI And zipToCensusTract \n",
    "    @param eji: environmental justice index dataframe\n",
    "    - if specific state, should already be a subdataframe\n",
    "    @param zipT: zip to census tract conversion (subdataframe if state)\n",
    "    @return merged dataframe\n",
    "    \"\"\"\n",
    "    cols = eji.columns[10:]\n",
    "    ejiDict = eji.set_index('GEOID', drop=True)[cols].to_dict(orient=\"index\")\n",
    "    \n",
    "    ## Sets up the ZIP to TRACT conversion\n",
    "    zipDF = zipT[['TRACT','ZIP','USPS_ZIP_PREF_STATE']]\n",
    "    zipDict = {}\n",
    "    zipCity = {}\n",
    "    for i in range(len(zipDF.index)):\n",
    "        temp = zipDF.iloc[i]\n",
    "        tempZip = temp['ZIP']\n",
    "        zipCity[tempZip] = temp['USPS_ZIP_PREF_STATE']\n",
    "        if tempZip in zipDict:\n",
    "            zipDict[tempZip].append(temp['TRACT'])\n",
    "        else:\n",
    "            zipDict[tempZip] = [temp['TRACT']]\n",
    "    \n",
    "    ## Averages the EJI data across a zipcode \n",
    "    total = []\n",
    "    for z in zipDict: \n",
    "        arrays = []\n",
    "        for key in zipDict[z]:\n",
    "            if key in ejiDict: \n",
    "                arrays.append(np.array(list(ejiDict[key].values())))\n",
    "        meanArray = [np.mean(k) for k in zip(*arrays)]\n",
    "        total.append(meanArray)\n",
    "    \n",
    "    merged = pd.DataFrame(total)\n",
    "    merged.columns = cols\n",
    "    merged['ZIP'] = list(zipDict.keys())\n",
    "    \n",
    "    ## Adds state into the data too \n",
    "    states = []\n",
    "    for m in merged['ZIP']:\n",
    "        states.append(zipCity[m])\n",
    "        \n",
    "    merged['STATE'] = states\n",
    "    \n",
    "    return merged.dropna()\n",
    "\n",
    "def plotlyZip(state):\n",
    "    \"\"\"\n",
    "    Gets the plotly data for zipcodes based on a specific state\n",
    "    @param state: capitalized state string\n",
    "    @return zipcodes: JSON of zipcode data\n",
    "    \"\"\"\n",
    "\n",
    "    folder = \"Data/State-zip-code-GeoJSON/\"\n",
    "    stateFile = \"\"\n",
    "    for file in os.listdir(folder):\n",
    "        if file.startswith(state.lower()):\n",
    "            stateFile = file\n",
    "    with open(folder+stateFile) as f:\n",
    "        zipcodes = json.load(f)\n",
    "    return zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6abe7",
   "metadata": {},
   "source": [
    "### These are the defined columns for the EJI that we use for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "health = [\"EP_BPHIGH\",\"EP_ASTHMA\",\"EP_CANCER\",\"EP_MHLTH\",\"EP_DIABETES\",\"EPL_BPHIGH\",\n",
    "          \"EPL_ASTHMA\",\"EPL_CANCER\",\"EPL_DIABETES\",\"EPL_MHLTH\",\"F_BPHIGH\",\"F_ASTHMA\",\n",
    "          \"F_CANCER\",\"F_MHLTH\",\"F_DIABETES\"]\n",
    "env = [\"EPL_OZONE\",\"EPL_PM\",\"EPL_DSLPM\",\"EPL_TOTCR\",\"SPL_EBM_THEME1\",\"RPL_EBM_DOM1\",\n",
    "    \"EPL_NPL\",\"EPL_TRI\",\"EPL_TSD\",\"EPL_RMP\",\"EPL_COAL\",\"EPL_LEAD\",\"SPL_EBM_THEME2\",\n",
    "    \"RPL_EBM_DOM2\",\"EPL_PARK\",\"EPL_HOUAGE\",\"EPL_WLKIND\",\"SPL_EBM_THEME3\",\"RPL_EBM_DOM3\",\n",
    "    \"EPL_RAIL\",\"EPL_ROAD\",\"EPL_AIRPRT\",\"SPL_EBM_THEME4\",\"RPL_EBM_DOM4\",\"EPL_IMPWTR\",\n",
    "    \"SPL_EBM_THEME5\",\"RPL_EBM_DOM5\",\"E_OZONE\",\"E_PM\",\"E_DSLPM\",\"E_TOTCR\",\"E_NPL\",\"E_TRI\",\n",
    "    \"E_TSD\",\"E_RMP\",\"E_COAL\",\"E_LEAD\",\"E_PARK\",\"E_HOUAGE\",\"E_WLKIND\",\"E_RAIL\",\"E_ROAD\",\n",
    "    \"E_AIRPRT\",\"E_IMPWTR\"]\n",
    "soc = [\"EPL_MINRTY\",\"SPL_SVM_DOM1\",\"RPL_SVM_DOM1\",\"EPL_POV200\",\"EPL_NOHSDP\",\"EPL_UNEMP\",\n",
    "    \"EPL_RENTER\",\"EPL_HOUBDN\",\"EPL_UNINSUR\",\"EPL_NOINT\",\"SPL_SVM_DOM2\",\"RPL_SVM_DOM2\",\n",
    "    \"EPL_AGE65\",\"EPL_AGE17\",\"EPL_DISABL\",\"EPL_LIMENG\",\"SPL_SVM_DOM3\",\"RPL_SVM_DOM3\",\n",
    "    \"EPL_MOBILE\",\"EPL_GROUPQ\",\"SPL_SVM_DOM4\",\"RPL_SVM_DOM4\",\"EP_MINRTY\",\"EP_POV200\",\n",
    "    \"EP_NOHSDP\",\"EP_UNEMP\",\"EP_RENTER\",\"EP_HOUBDN\",\"EP_UNINSUR\",\"EP_NOINT\",\"EP_AGE65\",\n",
    "    \"EP_AGE17\",\"EP_DISABL\",\"EP_LIMENG\",\"EP_MOBILE\",\"EP_GROUPQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs are: EJI File, zipT file, and state name abbreviation\n",
    "eji = pd.read_csv(\"Data/ejiData.csv\")\n",
    "zipT = pd.read_csv(\"Data/TRACT_ZIP_122023.csv\")\n",
    "state = stateName.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes = plotlyZip(state)\n",
    "zipSub = zipT[zipT['USPS_ZIP_PREF_STATE']==state]\n",
    "ejiSub = eji[eji['StateAbbr']==state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ecbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the final data set\n",
    "zipEJI = mergeDFs(ejiSub, zipSub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb7499",
   "metadata": {},
   "source": [
    "## Step 3: Spatial Correlation with Local Moran's (Method 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11461e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialCorr(zipEJI, cols, zipcodes, state):\n",
    "    \"\"\"\n",
    "    Runs Local Moran's spatial geoclustering\n",
    "    @param zipEJI: dataframe of zipcode-EJI merged\n",
    "    @param cols: columns of choice (default is ALL)\n",
    "    @param zipcodes: zipcode JSON dict for choropleth\n",
    "    @param state: state name \n",
    "    @return a couple figures (choropleth, weights, local morans)\n",
    "    \"\"\"\n",
    "    filename = \"Data/zipcodeXY.csv\"\n",
    "    zipFile = pd.read_csv(filename)\n",
    "\n",
    "    df = pd.merge(zipEJI, zipFile, on='ZIP')\n",
    "\n",
    "    # Drop rows with missing lat/lon\n",
    "    df = df.dropna(subset=['LAT', 'LNG'])\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(df['LNG'], df['LAT'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "\n",
    "    # Optionally set the coordinate reference system (CRS) to WGS84 (EPSG:4326)\n",
    "    gdf.set_crs(epsg=32130, inplace=True)\n",
    "\n",
    "    w = weights.contiguity.Queen.from_dataframe(gdf, use_index=False)\n",
    "\n",
    "    plot_spatial_weights(w, gdf)\n",
    "    plt.savefig(\"Figures/%s/%s_spatialWeights.png\" %(state, state))\n",
    "\n",
    "    ## LOCAL SPATIAL AUTOCORRELATION\n",
    "\n",
    "    # Transforming weights into binary (if it's 1 = is neighbor, 0 = not neighbor)\n",
    "    w.transform = \"B\"\n",
    "\n",
    "    x = PCA(n_components=1).fit_transform(df[cols+[\"LAT\",\"LNG\"]].to_numpy())[:,0]\n",
    "    df['PCA'] = x\n",
    " \n",
    "    # Local Moran's I\n",
    "    pop_count_local_moran = Moran_Local(x, w)\n",
    "    moran = Moran(x, w)\n",
    "    print(\"Local Morans Results: \")\n",
    "    print(\"Moran Statistic:\", moran.I)\n",
    "    print(\"Ranges from -1 to 1, closer to 1 indicates spatial clustered variable\")\n",
    "    print(\"Morans P-value:\",moran.p_sim)\n",
    "    print(\"If the p-value is below 0.05, we can reject null hypothesis and interpret this variable is clustered spatially.\")\n",
    "    print()\n",
    "\n",
    "    # Plotting Local Moran's I scatterplot of pop_count\n",
    "    fig, ax = moran_scatterplot(pop_count_local_moran, p=0.05)\n",
    "\n",
    "    plt.text(1.95, 1, 'HH', fontsize=25)\n",
    "    plt.text(1.95, -1.0, 'HL', fontsize=25)\n",
    "    plt.text(-1.5, 1, 'LH', fontsize=25)\n",
    "    plt.text(-1.5, -1, 'LL', fontsize=25)\n",
    "    plt.savefig(\"Figures/%s/%s_localmorans.png\" %(state, state))\n",
    "\n",
    "    # creating column with local_moran classification\n",
    "    df['pop_count_local_moran'] = pop_count_local_moran.q\n",
    "\n",
    "    # Dict to map local moran's classification codes\n",
    "    local_moran_classification = {1: 'HH', 2: 'LH', 3: 'LL', 4: 'HL'}\n",
    "\n",
    "    # Mapping local moran's classification codes\n",
    "    df['pop_count_local_moran'] = df['pop_count_local_moran'].map(local_moran_classification)\n",
    "\n",
    "    # p-value for each observation/neighbor pair\n",
    "    df['pop_count_local_moran_p_sim'] = pop_count_local_moran.p_sim\n",
    "\n",
    "    # If p-value > 0.05 it is not statistical significant\n",
    "    df['pop_count_local_moran'] = np.where(df['pop_count_local_moran_p_sim'] > 0.05, 'ns', df['pop_count_local_moran'])\n",
    "\n",
    "    # creates the choropleth map \n",
    "    newZips = []\n",
    "    zips = df['ZIP'].values.tolist()\n",
    "    for i in range(len(zips)):\n",
    "        if len(str(zips[i]))==4:\n",
    "            newZips.append(\"0\"+str(zips[i]))\n",
    "        else:\n",
    "            newZips.append(str(zips[i]))\n",
    "    df['Zipcode'] = newZips\n",
    "    fig = px.choropleth(df, \n",
    "                    geojson=zipcodes, \n",
    "                    locations='Zipcode', \n",
    "                    color='pop_count_local_moran',\n",
    "                    color_continuous_scale=\"Viridis\",\n",
    "                    range_color=(min(df['pop_count_local_moran']),max(df['pop_count_local_moran'])),\n",
    "                    featureidkey=\"properties.ZCTA5CE10\",\n",
    "                    scope=\"usa\",\n",
    "                    labels={'Cluster':'Cluster_Category'}\n",
    "                          )\n",
    "    fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "    fig.write_image(\"Figures/%s/%s_moranMap.png\" %(state, state), scale=2)\n",
    "    fig.show()\n",
    "\n",
    "    # run feature importance also \n",
    "    featureImportance(df[cols],df['pop_count_local_moran'].values.tolist(), 'Morans', state)\n",
    "\n",
    "    return w, gdf, df\n",
    "\n",
    "def featureImportance(X,y, method, state):\n",
    "    \"\"\"\n",
    "    Analyzes feature importance using a random forest model \n",
    "    @param X: the input data as a dataframe\n",
    "    @param y: the clusters \n",
    "    @param method: a string of the method name \n",
    "    @param state: state name\n",
    "    @return figure \"<method>_featureImp.png\" in Figures folder\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)  # Use 100 trees\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    importances = clf.feature_importances_\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "    fig = px.bar(feature_importance_df, x='Importance', y='Feature', orientation='h',\n",
    "             title='Top 10 Feature Importances')\n",
    "    fig.write_image(\"Figures/%s/%s_%s_featureImp.png\" %(state, state, method))\n",
    "    fig.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a356d32",
   "metadata": {},
   "source": [
    "## Step 5: Running spatial correlations and interpreting results \n",
    "\n",
    "The following results show up with this code: \n",
    "1. Local Moran's Results \n",
    "2. A map of local moran's clusters\n",
    "3. Top 10 feature importance of moran's clusters\n",
    "4. A network map of weights\n",
    "5. Local Moran's scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c25576",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Figures/%s\" %(state)):\n",
    "    os.mkdir(\"Figures/%s\" %(state))\n",
    "w, gdf, df = spatialCorr(zipEJI, health+env+soc, zipcodes, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc7ec7",
   "metadata": {},
   "source": [
    "## Step 6: Network Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ff5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(w, gdf, cols, df, zipcodes, state):\n",
    "    \"\"\"\n",
    "    Network based clustering using markovian clustering\n",
    "    @param w: weights object from Queen \n",
    "    @param gdf: pandas geodataframe\n",
    "    @param cols: columns of choice (default is ALL)\n",
    "    @param df: pandas dataframe of gdf\n",
    "    @param zipcodes: zipcodes dictionary for choropleth\n",
    "    @param state: state input \n",
    "    \"\"\"\n",
    "    # Step 1: creates new networkx graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for region, neighbors in w.neighbors.items(): \n",
    "        for neighbor in neighbors: \n",
    "            G.add_edge(region, neighbor)\n",
    "\n",
    "    for idx, row in gdf.iterrows(): \n",
    "        G.nodes[idx]['geometry'] = row['geometry']\n",
    "        for c in cols: \n",
    "            G.nodes[idx][c] = row[c]\n",
    "\n",
    "    adj_matrix = nx.to_scipy_sparse_array(G)\n",
    "    sparse_csr_matrix = csr_matrix(adj_matrix)\n",
    "\n",
    "    # Step 2: Perform the Markov Clustering (MCL)\n",
    "    result = mc.run_mcl(sparse_csr_matrix)  # Run MCL algorithm\n",
    "    clusters = mc.get_clusters(result)  # Get clusters\n",
    "\n",
    "    # Step 3: Assign each node to its respective cluster\n",
    "    # Create a dictionary where the key is the node and value is the cluster number\n",
    "    cluster_dict = {}\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for node in cluster:\n",
    "            cluster_dict[node] = i\n",
    "\n",
    "    # Step 4: Define a color for each cluster\n",
    "    # Generate random colors for each cluster\n",
    "    num_clusters = len(clusters)\n",
    "    colors = ['#%06X' % np.random.randint(0, 0xFFFFFF) for _ in range(num_clusters)]\n",
    "\n",
    "    # Step 5: Extract node geometries (we will use centroids)\n",
    "    gdf['centroid'] = gdf.geometry.centroid\n",
    "    node_positions = {i: (geom.x, geom.y) for i, geom in gdf['centroid'].items()}\n",
    "\n",
    "    # Step 6: Prepare node traces for plotting\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_color = []\n",
    "    for node, (x, y) in node_positions.items():\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_color.append(colors[cluster_dict[node]]) \n",
    "\n",
    "    # Step 7: Prepare edge traces for plotting\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = node_positions[edge[0]]\n",
    "        x1, y1 = node_positions[edge[1]]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)  # To create breaks between segments\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)  # To create breaks between segments\n",
    "\n",
    "    # Step 8: Plot the edges\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=1, color='black'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "    # Step 9: Plot the nodes\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            color=node_color,\n",
    "            size=10,\n",
    "            line_width=2\n",
    "        ),\n",
    "        text=[f\"Node {i} - Cluster {cluster_dict[i]}\" for i in G.nodes()]  # Optional: Add hover text for each node\n",
    "    )\n",
    "\n",
    "    # Step 10: Combine both node and edge traces into a Plotly figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace])\n",
    "\n",
    "    # Step 11: Update layout for better display\n",
    "    fig.update_layout(\n",
    "        title=\"NetworkX Graph with Node Geometries\",\n",
    "        titlefont_size=16,\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=0, l=0, r=0, t=0),\n",
    "        xaxis=dict(showgrid=False, zeroline=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False)\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.write_image(\"Figures/%s/%s_network.png\" %(state, state), scale=2)\n",
    "    fig.show()\n",
    "\n",
    "    # Step 12: Create the choropleth map \n",
    "    clusters = []\n",
    "    for idx, row in gdf.iterrows(): \n",
    "        clusters.append(cluster_dict[idx])\n",
    "    df['MCL'] = clusters\n",
    "    print(\"Markov Clustering: \")\n",
    "    print(\"Number of Clusters: \", max(clusters))\n",
    "    print()\n",
    "    fig2 = px.choropleth(df, \n",
    "                    geojson=zipcodes, \n",
    "                    locations='Zipcode', \n",
    "                    color='MCL',\n",
    "                    color_continuous_scale=\"Viridis\",\n",
    "                    range_color=(min(df['MCL']),max(df['MCL'])),\n",
    "                    featureidkey=\"properties.ZCTA5CE10\",\n",
    "                    scope=\"usa\",\n",
    "                    labels={'Cluster':'Cluster_Category'}\n",
    "                          )\n",
    "    fig2.update_geos(fitbounds=\"locations\", visible=False)\n",
    "    fig2.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "    fig2.write_image(\"Figures/%s/%s_mclMap.png\" %(state, state))\n",
    "    fig2.show()\n",
    "\n",
    "    # Step 13: Runs the feature importance also \n",
    "    featureImportance(df[cols],clusters, 'MCL', state)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a7292",
   "metadata": {},
   "source": [
    "## Step 7: Running Network Clustering\n",
    "\n",
    "The following results show up here: \n",
    "1. Network representation of the state\n",
    "2. MCL Clusters\n",
    "3. Top 10 features for MCL clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1646df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "network(w, gdf, health+env+soc, df, zipcodes, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b956551",
   "metadata": {},
   "source": [
    "## Step 8: Interpretation\n",
    "\n",
    "At this point, you can add any code of interest to understand more about these results if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd92a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
